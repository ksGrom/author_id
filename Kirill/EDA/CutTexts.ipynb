{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоматический поиск произведений в папках texts/txt/\\*\\* (вместо \\*\\* — имя автора) и создание словарей *название-путь\\_к\\_файлу* (TITLE_PATH_DICT), *название-автор* (TITLE_AUTHOR_DICT), *автор-множество\\_названий\\_произведений* (AUTHOR__TITLE_SET__DICT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATHS = glob.glob('texts/converted/**/*.txt', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_PATH_DICT = {path.split(\"\\\\\")[-1].split(\".\")[0] : path for path in FILE_PATHS}\n",
    "TITLE_AUTHOR_DICT = {path.split(\"\\\\\")[-1].split(\".\")[0] : path.split(\"\\\\\")[-2] for path in FILE_PATHS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR__TITLE_SET__DICT = collections.defaultdict(set)\n",
    "for title, author in TITLE_AUTHOR_DICT.items():\n",
    "    AUTHOR__TITLE_SET__DICT[author].add(title)\n",
    "AUTHOR__TITLE_SET__DICT = dict(AUTHOR__TITLE_SET__DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация и токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_WORDS = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_FOLDER = 'texts/5000_words_lemmatized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "patterns = \"[A-Za-z0-9!#$%&'()*+,.…/:;<=>?@[\\]^_`{|}~—–\\\"«»„“\\-]+\"\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def lemmatize(doc, remove_stop_words=False):\n",
    "    doc = re.sub(patterns, ' ', doc)\n",
    "    tokens = []\n",
    "    stopwords_ru = stopwords.words(\"russian\") if remove_stop_words else ''\n",
    "    for token in doc.split():\n",
    "        if token and token.strip() not in stopwords_ru:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            \n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [13:43<00:00,  6.29s/it]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(TITLE_PATH_DICT)) as pbar:\n",
    "    for author, title_set in AUTHOR__TITLE_SET__DICT.items():\n",
    "        i = 0\n",
    "        lemmatized = []\n",
    "        new_dir = os.path.join(NEW_FOLDER, author) \n",
    "        if not os.path.exists(new_dir):\n",
    "            os.makedirs(new_dir)\n",
    "        for title in title_set:\n",
    "            path = TITLE_PATH_DICT[title]\n",
    "            with open(path, 'r', encoding=\"utf8\") as infile:\n",
    "                text = infile.read()\n",
    "            lemmatized += lemmatize(text)[:-250]\n",
    "            while len(lemmatized) >= NUM_OF_WORDS:\n",
    "                np.save(os.path.join(new_dir, f\"{i}.npy\"), lemmatized[:NUM_OF_WORDS])\n",
    "                del lemmatized[:NUM_OF_WORDS] \n",
    "                i += 1\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нарезка по N предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_SENTENCES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_FOLDER = f'texts/{NUM_OF_SENTENCES}_sentences/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_of_sentence_end(string, start=0):\n",
    "    ind = start\n",
    "    for char in string[start:]:\n",
    "        if char in \".?!…\":\n",
    "            return ind\n",
    "        ind += 1\n",
    "    return None\n",
    "\n",
    "def string_to_list_of_sentences(string):\n",
    "    sentences = []\n",
    "    while len(string) > 0:\n",
    "        end_ind = get_ind_of_sentence_end(string)\n",
    "        if end_ind is None:\n",
    "            break\n",
    "        sentence = string[:end_ind+1]\n",
    "        if len(sentence) == 1 and len(sentences) > 0:\n",
    "            sentences[-1] = sentences[-1] + sentence\n",
    "        else:\n",
    "            sentences.append(sentence)\n",
    "        string = string[end_ind+1:]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [02:07<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(TITLE_PATH_DICT)) as pbar:\n",
    "    for author, title_set in AUTHOR__TITLE_SET__DICT.items():\n",
    "        i = 0\n",
    "        sentences = []\n",
    "        new_dir = os.path.join(NEW_FOLDER, author) \n",
    "        if not os.path.exists(new_dir):\n",
    "            os.makedirs(new_dir)\n",
    "        for title in title_set:\n",
    "            path = TITLE_PATH_DICT[title]\n",
    "            with open(path, 'r', encoding=\"utf8\") as infile:\n",
    "                text = infile.read()\n",
    "            sentences += string_to_list_of_sentences(text)[:-10]\n",
    "            while len(sentences) >= NUM_OF_SENTENCES:\n",
    "                np.save(os.path.join(new_dir, f\"{i}.npy\"), sentences[:NUM_OF_SENTENCES])\n",
    "                del sentences[:NUM_OF_SENTENCES] \n",
    "                i += 1\n",
    "            pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
